{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Wikipedia Example\n",
    "\n",
    "This example goes through how to create an minDB object for the entirety of English Wikipedia. After compression, the trained faiss index only takes up ~1.5GB, which can easily be held in memory on any computer (with the parameters set in this example). The full vectors and text take up 150GB on disk.\n",
    "\n",
    "#### Pre-requisites:\n",
    "1. You must have at least 250GB of space on disk. Even though the final result only takes up ~150GB, during the download step, you end up with the raw downloaded files as well as post-processed files (these are the .arrow files). This is why there is a step to remove the folder where all of the downloads end up, since they are no longer needed.\n",
    "2. `datasets` and `pyarrow`. These can be installed with `pip install pyarrow datasets`.\n",
    "3. A Cohere API key (only needed for making a test query). You can get one for free [here](https://dashboard.cohere.ai/welcome/register)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Load in the necessary packages and append the paths needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset # Run pip install datasets\n",
    "import pyarrow as pa # Run pip install pyarrow\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Load in minDB from the local directory\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(current_dir + \"/../\")\n",
    "\n",
    "from mindb.mindb import minDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define some helper functions for reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###\n",
    "def read_embeddings(data):\n",
    "    all_embeddings = [data[\"emb\"][i] for i in range(data.shape[0])]\n",
    "    return all_embeddings\n",
    "\n",
    "def read_text(data):\n",
    "    all_text = [data['text'][i] for i in range(data.shape[0])]\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "Download the datasets from HuggingFace. This will take ~30-60 minutes depending on your internet connection.\n",
    "\n",
    "THIS WILL TAKE UP ~250GB OF SPACE ON DISK. PLEASE MAKE SURE YOU HAVE THIS MUCH SPACE BEFORE PROCEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_dataset(\"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the filepath where the data was saved (this will be printed out above when the download is complete)\n",
    "# It should be something like \"/Users/{username}/.cache/huggingface/datasets/Cohere___parquet/...\"\n",
    "filepath = \"/Users/{username}/.cache/huggingface/datasets/Cohere___parquet/...\"\n",
    "files = os.listdir(filepath)\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the downloads folder, since it is no longer needed\n",
    "# The filepath for that should be something like \"/Users/{username}/.cache/huggingface/datasets/downloads\"\n",
    "\n",
    "### PLEASE CONFIRM YOU HAVE THE CORRECT FILEPATH BEFORE RUNNING ###\n",
    "import shutil\n",
    "download_dir = '/Users/{username}/.cache/huggingface/datasets/downloads'\n",
    "shutil.rmtree(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the minDB object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the minDB object\n",
    "db_name = 'wikipedia_database'\n",
    "db = minDB(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add data to the minDB object\n",
    "\n",
    "This section parses each file that was downloaded from Cohere to get the embeddings and text. It then creates the list of tuples needed for the `db.add()` method.\n",
    "\n",
    "If you want to test this on a smaller set of the data first, you can set `max_files` to a smaller number (25, for example, would add ~10% of the data to the minDB object). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in each file and add the vectors and text to an minDB object (this takes ~45-60 minutes for the entire dataset)\n",
    "\n",
    "# Optional - set a max number of files to read in (The entire wikipedia dataset is 252 files)\n",
    "max_files = 500 # initialize to a large number to read in everything\n",
    "\n",
    "for i,file in enumerate(files):\n",
    "    print (i)\n",
    "\n",
    "    if i >= max_files:\n",
    "        break\n",
    "\n",
    "    filename = os.path.join(filepath, file)\n",
    "    extension = filename.split('.')[-1]\n",
    "    # We only care about the .arrow files\n",
    "    if extension != 'arrow':\n",
    "        continue\n",
    "    mmapped_file = pa.memory_map(filename, 'r')\n",
    "    reader = pa.ipc.open_stream(mmapped_file)\n",
    "    table = reader.read_all()\n",
    "    data = table.to_pandas()\n",
    "\n",
    "    embeddings = read_embeddings(data)\n",
    "    text = read_text(data)\n",
    "\n",
    "    add_data = [(embeddings[i], {\"text\": text[i]}) for i in range(len(embeddings))]\n",
    "    db.add(add_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the minDB object\n",
    "\n",
    "This should take 3-4 hours to train the entire dataset\n",
    "\n",
    "Make sure your computer doesn't go to sleep, or the training will pause\n",
    "\n",
    "For more information on these parameters, you can visit the Github Wiki [here](https://github.com/D-Star-AI/minDB/wiki/Tunable-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.train(True, pca_dimension=256, compressed_vector_bytes=32, omit_opq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the minDB\n",
    "\n",
    "You can ask anything you like here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"ENTER_YOUR_API_KEY_HERE\")\n",
    "\n",
    "query = [\"Who was the founder of YouTube?\"]\n",
    "embeddings = co.embed(query, model=\"embed-multilingual-v2.0\").embeddings\n",
    "query_embedding = embeddings[0] / np.linalg.norm(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the index\n",
    "results = db.query(query_embedding)\n",
    "print (results[\"metadata\"][0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6493f77fe247bf2d19f0ba28dd5345ab8e8eb3b6587168c5c28be0d535e3568d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
