{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import chromadb\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from mindb.mindb import minDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data_path = os.path.abspath(os.path.join('..', 'eval/data/fiqa_data.pickle'))\n",
    "with open(data_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "corpus_embeddings = data[\"corpus_embeddings\"]\n",
    "query_embeddings = data[\"query_embeddings\"]\n",
    "ground_truths = data[\"ground_truths\"]\n",
    "text = data[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the minDB object\n",
    "\n",
    "The data is added as a list of tuples containing (vector, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the minDB object\n",
    "db = minDB(\"fiqa_eval_test\", create_or_load=\"load\")\n",
    "\n",
    "# Add the embeddings and text to the database\n",
    "data = [(corpus_embeddings[i], {\"text\": text[i]}) for i in range(len(corpus_embeddings))]\n",
    "#db.add(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the minDB object\n",
    "\n",
    "We set the parameters to be a good compromise of performance and compression. More detail about each parameter can be found [here](\"https://github.com/D-Star-AI/minDB/wiki/Tunable-parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dimension = 256\n",
    "opq_dimension = 128\n",
    "compressed_vector_bytes = 32\n",
    "omit_opq = False\n",
    "\n",
    "db.train(pca_dimension=pca_dimension, opq_dimension=opq_dimension, compressed_vector_bytes=compressed_vector_bytes, omit_opq=omit_opq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions to evaluate the retrieval performance\n",
    "\n",
    "Recall is defined as the `top_k` number of vectors retrieved that exist in the ground truth `top_k`.\n",
    "For example, if you retrieve 10 vectors, and 9 of them exist in the ground truth top 10, recall would be 0.9.\n",
    "\n",
    "Latency is defined as the latency for a single query, in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(db, queries: np.ndarray, ground_truths: np.ndarray, preliminary_top_k: int, final_top_k: int) -> tuple[float, float]:\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_sum = 0\n",
    "    for i in range(queries.shape[0]):\n",
    "        results = db.query(queries[i], preliminary_top_k, final_top_k)\n",
    "        reranked_I = results[\"ids\"]\n",
    "\n",
    "        # compute recall\n",
    "        total_sum += sum([1 for x in reranked_I[:final_top_k] if x in ground_truths[i, :final_top_k]]) # / final_top_k\n",
    "\n",
    "    end_time = time.time()\n",
    "    recall = total_sum / (ground_truths.shape[0] * final_top_k)\n",
    "    latency = (end_time - start_time) * 1000 / queries.shape[0] # latency per query in ms\n",
    "\n",
    "    return recall, latency\n",
    "\n",
    "\n",
    "def evaluate_chroma(collection, queries: np.ndarray, ground_truths: np.ndarray, top_k: int) -> tuple[float, float]:\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_sum = 0\n",
    "    for i in range(queries.shape[0]):\n",
    "        results = collection.query(\n",
    "            query_embeddings=[queries[i].tolist()],\n",
    "            n_results=top_k,\n",
    "            include=[\"metadatas\", \"distances\"] # Matches what is returned by minDB\n",
    "        )\n",
    "        reranked_I = results[\"ids\"][0]\n",
    "        # Convert each id to an integer (They are required to be strings in Chroma)\n",
    "        reranked_I = [int(x) for x in reranked_I]\n",
    "\n",
    "        # compute recall\n",
    "        total_sum += sum([1 for x in reranked_I[:top_k] if x in ground_truths[i, :top_k]]) / top_k\n",
    "\n",
    "    end_time = time.time()\n",
    "    recall = total_sum / (ground_truths.shape[0])\n",
    "    latency = (end_time - start_time) * 1000 / queries.shape[0] # latency per query in ms\n",
    "\n",
    "    return recall, latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the retrieval performance\n",
    "\n",
    "We are going to use a `preliminary_top_k` of 200, and a `final_top_k` of 20. `preliminary_top_k` is the number of results returned from the search over the compressed faiss index. Then a brute force search is run on those vectors (the full, uncompressed vectors this time) to get the `final_top_k` vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.9945987654320988\n",
      "latency:  5.17103554290018\n"
     ]
    }
   ],
   "source": [
    "preliminary_top_k = 200\n",
    "final_top_k = 20\n",
    "recall, latency = evaluate(\n",
    "    db, query_embeddings, ground_truths, preliminary_top_k, final_top_k\n",
    ")\n",
    "print (\"recall: \", recall)\n",
    "print (\"latency: \", latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the chromaDB client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09-19 10:46:09 chromadb.telemetry.product.posthog:20 in __init__() INFO     Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "path = os.path.abspath(os.path.join('..', 'eval/chromadb/'))\n",
    "client = chromadb.PersistentClient(path=path)\n",
    "\n",
    "# Create the collection\n",
    "#collection = client.create_collection(\"fiqa_eval_test\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "collection = client.get_collection(\"fiqa_eval_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into chunks of 10,000 vectors (ChromaDB has a limit on the number of vectors per add call)\n",
    "chunk_size = 10000\n",
    "for i in range(0, len(corpus_embeddings), chunk_size):\n",
    "    chunk = corpus_embeddings[i:i+chunk_size]\n",
    "    # Create the chunk ids. These need to match the indices of the vectors in the chunk\n",
    "    chunk_ids = [f\"{j}\" for j in range(i, i+len(chunk))]\n",
    "    # Create the metadata\n",
    "    metadata = [{\"text\": text[j]} for j in range(i, i+len(chunk))]\n",
    "    collection.add(\n",
    "        embeddings=chunk,\n",
    "        metadatas=metadata,\n",
    "        ids=chunk_ids\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the retrieval performance\n",
    "\n",
    "We are using the same recall and latency measures as with minDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.9218364197530848\n",
      "latency:  4.14560643243201\n"
     ]
    }
   ],
   "source": [
    "recall, latency = evaluate_chroma(\n",
    "    collection, query_embeddings, ground_truths, top_k=20\n",
    ")\n",
    "print (\"recall: \", recall)\n",
    "print (\"latency: \", latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
